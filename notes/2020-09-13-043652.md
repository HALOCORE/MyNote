# NLP

> NLP / Transformer

## [RES] Videos

Attention is All you need

https://www.youtube.com/watch?v=iDulhoQ2pro&ab_channel=YannicKilcher

Bert Explained!

https://www.youtube.com/watch?v=OR0wfP2FD3c&ab_channel=HenryAILabs

LSTM is dead. Long Live Transformers

https://www.youtube.com/watch?v=S27pHKBEp30

Pytorch Transformers from Scratch

https://www.youtube.com/watch?v=U0s0f995w14

## [RES] Pages

The Illustrated Transformer

http://jalammar.github.io/illustrated-transformer/

BERT Explained - A list of frequent asked questions

https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/

[Quora] Are transformers (BERT, etc.) one kind of embedding (word vectors)?

https://www.quora.com/Are-transformers-BERT-etc-one-kind-of-embedding-word-vectors

[Wikipedia] Transformer

https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)

Transformer Neural Network Architecture (a summary)

https://devopedia.org/transformer-neural-network-architecture

## Attention is All you need

## BERT Explained!



## LSTM is dead. Long Live Transformers

![image-20200913200309536](2020-09-13-043652.assets/image-20200913200309536.png)

- https://github.com/huggingface/transformers
  - both PyTorch and TensorFlow
  - Pre-trained models
  - Easy to fine-tune

![image-20200913201019001](2020-09-13-043652.assets/image-20200913201019001.png)

References
LSTM paper (scanned from stone tablets): https://www.bioinf.jku.at/publications/older/2604.pdf
LSTM diagrams, and a great deep dive: https://colah.github.io/posts/2015-08-Understanding-LSTMs/
Attention is all you need: https://arxiv.org/abs/1706.03762
Attention illustrated: https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3
BERT: https://arxiv.org/abs/1810.04805
ELMo: https://arxiv.org/pdf/1802.05365.pdf
Pre-trained transformer library: https://github.com/huggingface/transformers

## Illustrated Guide to Transformers Neural Network: A step by step explanation

https://www.youtube.com/watch?v=4Bdc55j80l8

![image-20200913204759293](2020-09-13-043652.assets/image-20200913204759293.png)

## Pytorch Transformers from Scratch

https://www.youtube.com/watch?v=U0s0f995w14

## 吴恩达 Sequence Models (C5W3L07 Attention Model Intuition)(C5W3L08 Attention Model)

https://www.youtube.com/watch?v=SysgYptB198&list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6&index=4

https://www.youtube.com/watch?v=quoGRI-1l0A&list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6&index=5

## (Hung-yi Lee) Transformer

https://www.youtube.com/watch?v=ugWDIIOHtPA

![image-20200914203307836](2020-09-13-043652.assets/image-20200914203307836.png)

![image-20200914203420017](2020-09-13-043652.assets/image-20200914203420017.png)

![image-20200914203527626](2020-09-13-043652.assets/image-20200914203527626.png)

![image-20200914203724469](2020-09-13-043652.assets/image-20200914203724469.png)

![image-20200914203800870](2020-09-13-043652.assets/image-20200914203800870.png)

![image-20200914203859394](2020-09-13-043652.assets/image-20200914203859394.png)

![image-20200914204013732](2020-09-13-043652.assets/image-20200914204013732.png)

![image-20200914204235762](2020-09-13-043652.assets/image-20200914204235762.png)

![image-20200914204417772](2020-09-13-043652.assets/image-20200914204417772.png)

![image-20200914204619179](2020-09-13-043652.assets/image-20200914204619179.png)

![image-20200914205249812](2020-09-13-043652.assets/image-20200914205249812.png)

![image-20200914205650040](2020-09-13-043652.assets/image-20200914205650040.png) 

![image-20200914212931067](2020-09-13-043652.assets/image-20200914212931067.png)

## (Hung-yi Lee) ELMO, BERT, GPT (Putting words into computers)

https://www.youtube.com/watch?v=UYPa347-DdE

![image-20200914214111411](2020-09-13-043652.assets/image-20200914214111411.png)

![image-20200914214300031](2020-09-13-043652.assets/image-20200914214300031.png)

![image-20200914214510770](2020-09-13-043652.assets/image-20200914214510770.png)

![image-20200914214555851](2020-09-13-043652.assets/image-20200914214555851.png)

![image-20200914214804281](2020-09-13-043652.assets/image-20200914214804281.png)

![image-20200914214903156](2020-09-13-043652.assets/image-20200914214903156.png)

![image-20200914215125471](2020-09-13-043652.assets/image-20200914215125471.png)

多层的RNN会突出多个embedding，最终的embedding是$\alpha_i$加权之和，所加的权重是和目标任务一起学习出来。

![image-20200914215327743](2020-09-13-043652.assets/image-20200914215327743.png)

![image-20200915003727621](2020-09-13-043652.assets/image-20200915003727621.png)

![image-20200914225650886](2020-09-13-043652.assets/image-20200914225650886.png)

两种approach一起使用。

![image-20200914232923315](2020-09-13-043652.assets/image-20200914232923315.png)

![image-20200915001226048](2020-09-13-043652.assets/image-20200915001226048.png)

![image-20200914233230230](2020-09-13-043652.assets/image-20200914233230230.png)

![image-20200914233415218](2020-09-13-043652.assets/image-20200914233415218.png)

![image-20200914234140168](2020-09-13-043652.assets/image-20200914234140168.png)

![image-20200915020138863](2020-09-13-043652.assets/image-20200915020138863.png)

![image-20200915020208976](2020-09-13-043652.assets/image-20200915020208976.png)

![image-20200915020753761](2020-09-13-043652.assets/image-20200915020753761.png)

![image-20200915022030451](2020-09-13-043652.assets/image-20200915022030451.png)

![image-20200915022337153](2020-09-13-043652.assets/image-20200915022337153.png)

## Text Preprocessing | Sentiment Analysis with BERT using huggingface, PyTorch and Python Tutorial

